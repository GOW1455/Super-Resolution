{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6306944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, PReLU, Conv2DTranspose\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FSRCNN(scale=4, d=56, s=12, m=4, input_shape=(None, None, 1)):\n",
    "    x_input = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(d, kernel_size=5, padding='same')(x_input)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    x = Conv2D(s, kernel_size=1, padding='same')(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    for _ in range(m):\n",
    "        x = Conv2D(s, kernel_size=3, padding='same')(x)\n",
    "        x = PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    x = Conv2D(d, kernel_size=1, padding='same')(x)\n",
    "    x = PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    x = Conv2DTranspose(1, kernel_size=9, strides=scale, padding='same')(x)\n",
    "\n",
    "    return Model(inputs=x_input, outputs=x, name=\"FSRCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467daf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DIV2KDataset(Sequence):\n",
    "    def __init__(self, lr_dir, hr_dir, patch_size=48, batch_size=16, scale=4):\n",
    "        self.lr_files = sorted(glob(os.path.join(lr_dir, '*.png')))\n",
    "        self.hr_files = sorted(glob(os.path.join(hr_dir, '*.png')))\n",
    "        self.patch_size = patch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.scale = scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.lr_files) // self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_lr = self.lr_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_hr = self.hr_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        lr_batch = []\n",
    "        hr_batch = []\n",
    "\n",
    "        for lr_path, hr_path in zip(batch_lr, batch_hr):\n",
    "            hr = tf.io.decode_png(tf.io.read_file(hr_path), channels=1)\n",
    "            hr = tf.image.convert_image_dtype(hr, tf.float32)\n",
    "\n",
    "            hr_shape = tf.shape(hr)\n",
    "            hr_h = hr_shape[0]\n",
    "            hr_w = hr_shape[1]\n",
    "\n",
    "            crop_size = self.patch_size * self.scale\n",
    "\n",
    "            if hr_h < crop_size or hr_w < crop_size:\n",
    "                print(f\"⚠️ Skipping small image: {hr_path}\")\n",
    "                continue\n",
    "\n",
    "            # Crop from HR\n",
    "            hr_crop = tf.image.random_crop(hr, [crop_size, crop_size, 1])\n",
    "\n",
    "            # Downscale to LR\n",
    "            lr_crop = tf.image.resize(hr_crop, [self.patch_size, self.patch_size], method='bicubic')\n",
    "\n",
    "            lr_batch.append(lr_crop)\n",
    "            hr_batch.append(hr_crop)\n",
    "\n",
    "        return tf.stack(lr_batch), tf.stack(hr_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr_metric(y_true, y_pred):\n",
    "    return tf.image.psnr(y_true, y_pred, max_val=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 4\n",
    "patch_size = 48\n",
    "batch_size = 16\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lr = '../DIV2K/train_LR_X4/DIV2K_train_LR_bicubic/X4'\n",
    "train_hr = '../DIV2K/valid_HR/DIV2K_valid_HR'\n",
    "val_lr = '../DIV2K/valid_LR_X4/DIV2K_valid_LR_bicubic/X4'\n",
    "val_hr = '../DIV2K/train_HR/DIV2K_train_HR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DIV2KDataset(train_lr, train_hr, patch_size, batch_size, scale)\n",
    "val_gen = DIV2KDataset(val_lr, val_hr, patch_size, batch_size, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f58515",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FSRCNN(scale=scale, input_shape=(None, None, 1))\n",
    "model.compile(optimizer=Adam(1e-4), loss=MeanSquaredError(), metrics=[psnr_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_gen, validation_data=val_gen, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fsrcnn_DIV2K_x4.h5')\n",
    "print(\"Model saved as 'fsrcnn_DIV2K_x4.h5'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
